I have Oracle db running in my local.
I need to setup another DB which will act as Downstream Log Mining DB for the above oracle db.
How can I do so ?


Setting up a Downstream Log Mining Database for Oracle involves several steps. This setup is typically used in Oracle GoldenGate configurations to offload the workload of capturing redo data from the source database to another database. Here's a step-by-step guide to help you configure this:

### Prerequisites

1. **Oracle Software**: Ensure you have the necessary Oracle software installed on both the source and downstream systems.
2. **Networking**: Ensure both the source and downstream systems can communicate with each other over the network.
3. **Sufficient Resources**: Ensure that the downstream database has sufficient resources (CPU, memory, storage) to handle the workload.

### Step 1: Prepare the Source Database

1. **Enable Archivelog Mode**:
   ```sql
   ALTER DATABASE ARCHIVELOG;
   ```

2. **Enable Supplemental Logging**:
   ```sql
   ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;
   ```

3. **Create a User for GoldenGate**:
   ```sql
   CREATE USER ggs_admin IDENTIFIED BY password DEFAULT TABLESPACE users TEMPORARY TABLESPACE temp QUOTA UNLIMITED ON users;
   GRANT DBA TO ggs_admin;
   ```

### Step 2: Prepare the Downstream Database

1. **Install Oracle Database Software**: Install the same version of Oracle Database software on the downstream server.

2. **Create the Downstream Database**: Create a new database instance if it does not already exist.

3. **Configure Archive Log Mode and Supplemental Logging** (similar to the source database):
   ```sql
   ALTER DATABASE ARCHIVELOG;
   ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;
   ```

4. **Configure Initialization Parameters**:
   Set the necessary initialization parameters in the `init.ora` or `spfile` for the downstream database.
   ```sql
   ALTER SYSTEM SET enable_goldengate_replication=TRUE SCOPE=BOTH;
   ALTER SYSTEM SET log_archive_dest_2='SERVICE=source_db PRIMARY_ROLE=ENABLE LGWR ASYNC VALID_FOR=(ONLINE_LOGFILES,PRIMARY_ROLE)' SCOPE=BOTH;
   ```

5. **Create a User for GoldenGate**:
   ```sql
   CREATE USER ggs_admin IDENTIFIED BY password DEFAULT TABLESPACE users TEMPORARY TABLESPACE temp QUOTA UNLIMITED ON users;
   GRANT DBA TO ggs_admin;
   ```

### Step 3: Configure the Downstream Mining Database

1. **Configure TNSNAMES.ORA**:
   Ensure that the `tnsnames.ora` file on both the source and downstream systems have entries for each other's databases.

2. **Configure Log Transport Services**:
   Configure the log transport services on the source database to send redo logs to the downstream database.
   ```sql
   ALTER SYSTEM SET log_archive_dest_2='SERVICE=downstream_db LGWR ASYNC VALID_FOR=(ONLINE_LOGFILES,PRIMARY_ROLE) DB_UNIQUE_NAME=downstream_db';
   ```

### Step 4: GoldenGate Configuration

1. **Install Oracle GoldenGate**: Install Oracle GoldenGate on the downstream server.

2. **Create the Extract Process**:
    - Create a parameter file for the extract process.
    - Register the extract process with the database.

   Example parameter file (`extprm.prm`):
   ```
   EXTRACT ext1
   USERID ggs_admin, PASSWORD password
   RMTHOST downstream_host, MGRPORT 7809
   RMTTRAIL ./dirdat/lt
   TABLE hr.*;
   ```

   Register the extract process:
   ```bash
   ./ggsci
   ADD EXTRACT ext1, TRANLOG, BEGIN NOW
   ADD EXTTRAIL ./dirdat/lt, EXTRACT ext1
   ```

3. **Start the Extract Process**:
   ```bash
   ./ggsci
   START EXTRACT ext1
   ```

### Step 5: Verify the Setup

1. **Check the Log Transport and Apply**:
   Ensure that redo logs are being transported and applied to the downstream database.

   ```sql
   SELECT DEST_ID, STATUS, ERROR FROM V$ARCHIVE_DEST_STATUS WHERE DEST_ID = 2;
   ```

2. **Monitor GoldenGate Processes**:
   Use GoldenGate commands to monitor the status of the extract and replicate processes.

   ```bash
   ./ggsci
   INFO EXTRACT ext1
   ```

### Final Notes

- **Backup**: Always ensure you have a backup of your databases before making significant changes.
- **Documentation**: Refer to Oracle's official documentation for detailed instructions and additional configuration options.
- **Testing**: Thoroughly test the setup in a non-production environment before deploying it to production.

By following these steps, you should be able to configure a Downstream Log Mining Database for your Oracle environment. If you encounter specific issues, please provide the details, and I can help troubleshoot further.